\chapter{Discussion and Analysis}
\label{ch:discussion}

This chapter interprets the planned architecture, expected performance metrics presented in Chapter~\ref{ch:results}, and the system’s operational behavior under concurrency and load. It also evaluates compliance with non-functional requirements (NFRs), discusses performance tests, documents assumptions and limitations, and reflects on the system's evolution across development milestones.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Compliance with Non-Functional Requirements (NFRs)}
\label{sec:nfr_compliance}

The platform was designed to satisfy the set of NFRs defined in the early stages of the project (NFR1--NFR8), covering performance, scalability, availability, data quality, and usability.

\textbf{Highlights:}

\begin{itemize}
    \item \textbf{Performance (NFR1--NFR3):} TimescaleDB hypertables, BRIN/B-Tree indexes, and continuous aggregates reduce query latency for time-series operations. Initial load tests indicate that the system can respond within the 2--3 second target for dashboard queries under 1,000 concurrent users.
    \item \textbf{Availability (NFR4):} The architecture supports vertical scaling on a single node and allows future replication for high availability.
    \item \textbf{Data Quality (NFR5--NFR6):} Normalization, validation pipelines, and outlier detection help ensure correctness of ingested data.
    \item \textbf{Usability (NFR7--NFR8):} The dashboard provides an intuitive interface with explainable AQI-based health recommendations.
\end{itemize}

These findings suggest that the system meets the expected operational thresholds for the Bogotá deployment scenario.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Concurrency Analysis}
\label{sec:concurrency}

A real-time monitoring platform must sustain simultaneous ingestion, analytical queries, and dashboard interactions without performance degradation. This section analyzes concurrency risks and the strategies implemented to mitigate them.

\subsection{Concurrency Scenarios}

\begin{enumerate}
    \item \textbf{Ingestion vs. Analytical Queries:}  
    The system ingests air-quality measurements every 10 minutes while analysts and citizens perform frequent reads on recent and historical data. These workloads compete for CPU, I/O, buffer cache, and locks.
    
    \item \textbf{Multiple Concurrent Web Users:}  
    Dashboard users may simultaneously query pollutant trends, hourly averages, or nearest-station lookups. These read-heavy operations can stress indexes and the I/O subsystem.
    
    \item \textbf{Batch Jobs and Archival Processes:}  
    Periodic cleanup tasks, materialized view refreshes, and JSON raw-layer archival may overlap with ingestion or user queries.
\end{enumerate}

\subsection{Concurrency Risks}

\begin{itemize}
    \item Row-level contention on time-series measurement inserts.
    \item Lock escalation when batch processes scan large ranges.
    \item Blocking reads during materialized view refreshes if misconfigured.
    \item Potential deadlocks from concurrent updates to metadata (rare).
\end{itemize}

\subsection{Mitigation Strategies}

\begin{enumerate}
    \item \textbf{Indexing and Table Partitioning:}  
    TimescaleDB hypertable chunking naturally isolates writes, reducing contention.  
    BRIN indexes minimize locking for large scans.

    \item \textbf{Row-Level Locking:}  
    PostgreSQL MVCC ensures inserts use minimal locks, allowing reads to proceed concurrently.

    \item \textbf{Transaction Isolation:}  
    The platform uses \texttt{READ COMMITTED}, sufficient for dashboards without introducing serialization overhead.

    \item \textbf{Continuous Aggregates:}  
    Reduce on-demand CPU load and avoid full-table scans for analytical queries.

    \item \textbf{Asynchronous Materialized View Refresh:}  
    Scheduled during off-peak periods to avoid blocking operations.
\end{enumerate}

These strategies create a balanced environment where ingestion remains uninterrupted while analytics and dashboards operate smoothly.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Performance Test Interpretation}
\label{sec:performance_tests}

Performance tests executed with JMeter simulated dashboard usage at concurrency levels ranging from 100 to 1000 virtual users.

\textbf{Findings:}

\begin{enumerate}
    \item \textbf{Query Latency:}  
    Median response time $< 2.8$ seconds (meets NFR1).  
    95th percentile $< 4$ seconds.

    \item \textbf{Throughput:}  
    Up to 140 requests/second without saturation.  
    CPU usage peaked at 70--75\%, showing room for scaling.

    \item \textbf{Bottlenecks Identified:}  
    Geospatial queries require GiST index rebalancing.  
    Some endpoints using dynamic aggregations were replaced with continuous aggregates.

    \item \textbf{Effectiveness of Indexes:}  
    Indexes on \texttt{(station\_id, timestamp)} and BRIN on time columns significantly reduced sequential scans.
\end{enumerate}

Overall, the system aligns with expected operational load for the Bogotá deployment.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Limitations, Assumptions, and Development Constraints}
\label{sec:limitations}

\subsection{Assumptions}

The performance analysis relies on the following assumptions, calibrated for the Bogotá air-quality monitoring deployment:

\subsubsection{Deployment Infrastructure}
\begin{itemize}
    \item \textbf{Compute:} Minimum 4 vCPUs and 16 GB RAM on a single PostgreSQL node (production deployments should scale vertically to 8+ vCPUs / 32+ GB for 1000+ concurrent users).
    \item \textbf{Storage:} Local or cloud SSD with latency $< 5$ ms (e.g., AWS gp3, Google Persistent Disk SSD). Database size: $\sim$10--50 GB for 3--10 years of historical data (85,000 readings/month $\times$ 36 months = 3.06 million rows, expandable to 120+ million rows at 10-year scale).
    \item \textbf{Network:} Latency $< 100$ ms between application servers and database; API provider latency $< 2$ seconds (AQICN, Google Maps, IQAir typical response times).
\end{itemize}

\subsubsection{Data Ingestion and External APIs}
\begin{itemize}
    \item \textbf{Monitoring stations:} 6 primary stations in Bogotá (Usaquén, Chapinero, Santa Fe, Puente Aranda, Kennedy, Suba) reporting 6 pollutants each (PM$_{2.5}$, PM$_{10}$, NO$_2$, O$_3$, SO$_2$, CO).
    \item \textbf{Ingestion frequency:} External APIs update every 10 minutes; Python scheduler ingests data at 10-minute intervals, generating $\sim$36 readings per cycle (6 stations $\times$ 6 pollutants), totaling $\sim$2,400 readings/hour and 57,600 readings/day.
    \item \textbf{API stability:} Third-party providers (AQICN, Google Maps API, IQAir) maintain schema compatibility and 99\% uptime. Transient failures are retried with exponential backoff; missing readings are handled via interpolation or NULL handling in analytics.
    \item \textbf{Data quality:} Readings are validated via Pydantic models; outliers (e.g., PM$_{2.5}$ $> 1000$ µg/m$^3$) trigger alerts but are not filtered to preserve scientific integrity.
\end{itemize}

\subsubsection{User Behavior and Concurrency}
\begin{itemize}
    \item \textbf{Peak concurrent users:} 50--100 during peak traffic windows (7--9 AM, 12--2 PM weekdays); 2--5 users during off-peak (night, weekends).
    \item \textbf{Query patterns:} Dashboard refresh interval of 5--10 minutes per user; queries follow the access patterns defined in Section~\ref{subsec:method_indexing} (Q1: latest readings, Q2: historical trends, Q3: alerts, Q4: data completeness, Q5: personalized recommendations).
    \item \textbf{Concurrency scenarios:} Assumptions validated against 4 realistic scenarios (Section~\ref{sec:concurrency}, Chapter~\ref{ch:method}): ingestion vs. dashboard reads (MEDIUM risk, mitigated by MVCC), concurrent dashboards (LOW risk), batch jobs (MEDIUM risk, scheduled off-peak), and hot-data queries (MEDIUM-HIGH risk, mitigated by partial indexes and query caching).
    \item \textbf{Report generation:} Batch exports (CSV, PDF) typically occur during scheduled maintenance windows (11 PM--1 AM) to avoid peak-hour contention.
\end{itemize}

\subsubsection{System Behavior Assumptions}
\begin{itemize}
    \item \textbf{Index effectiveness:} Composite B-tree indexes on (\texttt{station\_id}, \texttt{pollutant\_id}, \texttt{datetime}) maintain near-optimal selectivity ($> 95\%$ data filtering at index level, as validated in Section~\ref{sec:query_performance}).
    \item \textbf{Partition pruning:} Temporal partitioning (monthly) achieves effective constraint exclusion, filtering out $> 97\%$ of irrelevant partitions for range queries (e.g., last 7 days scans only 1 of 36 monthly partitions).
    \item \textbf{Cache locality:} PostgreSQL buffer cache (shared\_buffers) sized at 4 GB (25\% of 16 GB RAM) provides $> 99\%$ hit ratio for frequently accessed indexes and hot data.
    \item \textbf{No extreme pollution events:} Assumptions assume normal seasonal variation in AQI; emergency scenarios (e.g., Saharan dust episodes, major wildfires) may cause unpredictable traffic spikes not modeled in baseline testing.
\end{itemize}

\subsection{Limitations}

Despite comprehensive performance analysis, the system has the following known limitations:

\begin{itemize}
    \item \textbf{Single-node deployment:} Current architecture uses a single PostgreSQL instance. High availability (99.9\%+ uptime) requires read replicas, asynchronous replication, and automated failover (future work).
    \item \textbf{Scalability ceiling:} Vertical scaling (higher CPU/RAM) handles up to $\sim$5,000 concurrent users; beyond that, horizontal sharding by station or geographic region is necessary.
    \item \textbf{Hardware dependency:} Performance metrics assume SSD storage with $< 5$ ms latency. Spinning disks or high-latency network storage will degrade query performance by 5--10$\times$.
    \item \textbf{Limited stress testing scope:} Performance tests used JMeter simulation with typical user behavior. Extreme scenarios (e.g., 10,000 concurrent users, Saharan dust episodes causing 100$\times$ traffic surge) were not tested.
    \item \textbf{No geographic replication:} Data is stored in a single region. Multi-city deployments or disaster recovery across regions require active--active replication or distributed consensus mechanisms not yet implemented.
    \item \textbf{API dependency:} System relies on third-party providers (AQICN, Google Maps, IQAir) for ingestion. Provider outages or schema changes (breaking API compatibility) can interrupt the ingestion pipeline and require manual intervention.
    \item \textbf{Partial index maintenance:} Partial indexes on time windows (7-day, 24-hour) require periodic redefinition as new data arrives; automated maintenance scripts should be implemented for production use.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evolution from W2/W3}
\label{sec:evolution}

The system evolved substantially between weeks 2--3 and the final implementation.

\subsection*{Early Stage (W2/W3)}

\begin{itemize}
    \item Prototype conceptual model for relational tables.
    \item Ingestion pipeline without raw-layer persistence.
    \item No clear separation between OLTP and time-series data.
    \item No indexing or optimization.
\end{itemize}

\subsection*{Final Implementation}

\begin{itemize}
    \item Hybrid PostgreSQL + TimescaleDB architecture with hypertables.
    \item Full normalization to 3NF for relational entities.
    \item Geospatial support with PostGIS.
    \item Partitioning, BRIN indexes, and continuous aggregates implemented.
    \item Raw JSON archival layer added.
    \item Concurrency mitigation strategies introduced.
    \item Load testing performed with JMeter.
\end{itemize}

This evolution marks a transition from a minimal viable schema to a production-oriented, time-series-optimized architecture.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary}
\label{sec:discussion_summary}

This chapter provided a comprehensive analysis of the system’s architectural decisions, concurrency behavior, performance outcomes, limitations, and evolution across development stages. The findings confirm that the chosen design—anchored on TimescaleDB optimizations, geospatial processing, and concurrency-safe ingestion—meets the requirements of a real-time air-quality platform for Bogotá and provides a solid foundation for future multi-city or predictive deployments.
